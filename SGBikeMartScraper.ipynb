{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping 'Apple Watch Series 4' from Carousell "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping with Beautifulsoup (Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup\n",
    "- __Python Package__ to parse HTML and XML content\n",
    "- __Object__ to parse data from content (HTML Parser)\n",
    "    - creates a parse tree for parsed pages that can be used to extract data from HTML\n",
    "- Functions/Methods to extract data from the content\n",
    "- https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "### Functions/Methods\n",
    " - .find_all()\n",
    "     - return all the items that are using the tags you searched for\n",
    "     - soup.find_all('a')\n",
    "         - <class 'bs4.element.ResultSet'>\n",
    "         - <a class=\"sister\" href=(#\"http://example.com/elsie\") id=\"link1\">Elsie</a>,\n",
    "         - <a class=\"sister\" href=(#\"http://example.com/lacie\") id=\"link2\">Lacie</a>,\n",
    "         - <a class=\"sister\" href=(#\"http://example.com/tillie\") id=\"link3\">Tillie</a>\n",
    "     \n",
    " - .find()\n",
    "     - <class 'bs4.element.Tag'>\n",
    "     - return only 1 item that you searched for\n",
    "     - find(id=\"link3\")\n",
    "         - <a class=\"link\" href=(#\"http://example.com/example3\") id=\"link3\">This returns just the matching element by ID</a>\n",
    " - .select()\n",
    "     - .select(\"p > a\")\n",
    "         - <class 'list'>\n",
    "         - <a class=\"sister\" href=(#\"http://example.com/elsie\") id=\"link1\">Elsie</a>,\n",
    "         - <a class=\"sister\" href=(#\"http://example.com/lacie\")  id=\"link2\">Lacie</a>,\n",
    "         - <a class=\"sister\" href=(#\"http://example.com/tillie\") id=\"link3\">Tillie</a>\n",
    " \n",
    "### Steps to scrape a website\n",
    "1. Identify the structure of the website\n",
    "    - look out for odd cases that requires additional function to handle\n",
    "2. Plan the flow on how you want to scrape the website\n",
    "    - Navigation among the pages\n",
    "3. Estimate the amount of data you will need to scrape\n",
    "    - As the data increase, the time to run the program will be longer\n",
    "    - likely to send more request to the webpage server \n",
    "4. 'Inspect element' to inspect the source code of the page\n",
    "    - Understand the HTML code and identify what data you need\n",
    "5. Writing the code\n",
    "    - Planning on how to store the data\n",
    "    - Planning the structure of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16/07/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install html5lib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17/07/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports done\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import requests\n",
    "import time\n",
    "import html5lib\n",
    "import pandas as pd\n",
    "\n",
    "print(\"imports done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-58-c97440df65e2>, line 121)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-58-c97440df65e2>\"\u001b[0;36m, line \u001b[0;32m121\u001b[0m\n\u001b[0;31m    description = desc_data.get_text().strip()\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to send request to page and parse html page into BeautifulSoup Object\n",
    "def request_page(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}\n",
    "    req = urllib.request.Request(url, headers=headers)\n",
    "    html = urllib.request.urlopen(req)\n",
    "    read_html = html.read()\n",
    "    page = BeautifulSoup(str(read_html,\"utf-8\"),  \"html.parser\")\n",
    "    return page\n",
    "\n",
    "count_pages = 1\n",
    "bike_list = [] #sore all the apple watch \n",
    "\n",
    "base = 'https://www.sgbikemart.com.sg/'\n",
    "url = 'https://www.sgbikemart.com.sg/listing/usedbikes/listing/'\n",
    "extension = '?page=401&'\n",
    "pindex=401\n",
    "\n",
    "#?page=401&\n",
    "#and int(pindex) < 1\n",
    "\n",
    "while extension != None :\n",
    "    \n",
    "    count_items = 0\n",
    "    bike_page = request_page(url+extension)\n",
    "    \n",
    "    all_bike = bike_page.select('div.row > div.col-lg-9 > div.pmd-card') \n",
    "    \n",
    "    print(\"Number of items: \", len(all_bike), \"Page: \", count_pages)\n",
    "    print(\"URL: \", url + extension)\n",
    "\n",
    "    header=None\n",
    "    \n",
    "    for index in all_bike:\n",
    "        bike_dict ={} #store all the info of an apple watch\n",
    "        \n",
    "        #print(\"Looping through the records\")\n",
    "        \n",
    "        item_tag ='pmd-card pmd-z-depth'\n",
    "        \n",
    "        ad_header, ad_price, ad_currency,ad_biketype,ad_registration, description,ad_posted_on,ad_direct_seller,ad_dealer_seller,ad_paid_flag,expired,ad_status,ad_linkdetails,ad_mileage='','','','','','','','','','','','','',''\n",
    "        \n",
    "        # Retrieve data from website using HTML tags\n",
    "             \n",
    "        header_tag= 'pmd-card-header text-center-xs'\n",
    "        header = index.find(name='div', class_=header_tag)\n",
    "        if header != None:\n",
    "            if header.find('a') != None:\n",
    "                ad_header = header.get_text().strip()\n",
    "                #print(\"***** Title : \" + ad_header + \" *****\")\n",
    "        \n",
    "        if ad_header != None:\n",
    "            #print(ad_header)\n",
    "            split_header = ad_header.split()\n",
    "                \n",
    "        body_tag ='pmd-card-body'\n",
    "        body_data= index.find(name='div', class_='col-lg-10')\n",
    "        \n",
    "        #print(body_data)\n",
    "        if body_data != None:\n",
    "            #for body_item in body_data:\n",
    "\n",
    "            list_price = body_data.find(name='strong', class_='listing-price')\n",
    "        \n",
    "            ad_keydata=body_data.find_all(name='div', class_='col-lg-3')\n",
    "        \n",
    "            for adkey in ad_keydata:\n",
    "                #\n",
    "                text= adkey.get_text().strip()\n",
    "                \n",
    "                #print(text)\n",
    "                \n",
    "                if text.startswith('SGD'):\n",
    "                    ad_price=text\n",
    "                    \n",
    "                if text.startswith('Reg'):\n",
    "                    ad_registration=text[len('Reg :'):].strip()\n",
    "                    \n",
    "                if text.startswith('Type'):\n",
    "                    ad_biketype=text[len('Type: '):].strip()\n",
    "                    \n",
    "            ad_keydata=body_data.find_all(name='div', class_='col-lg-4')\n",
    "            \n",
    "            for adkey in ad_keydata:\n",
    "                text= adkey.get_text().strip()\n",
    "                \n",
    "                #print(text)\n",
    "                            \n",
    "                if text.startswith('Posted'):\n",
    "                    ad_posted_on=text[len('Posted on :'):].strip()\n",
    "                    #print(\"POSTED \" + ad_posted_on)\n",
    "                    \n",
    "                if 'Paid' in text:\n",
    "                    ad_paid_flag=\"True\"\n",
    "                \n",
    "                if 'Direct' in text:\n",
    "                    ad_direct_seller=\"True\"\n",
    "                    \n",
    "                if 'Dealer' in text:\n",
    "                    ad_dealer_seller=\"True\"\n",
    "                 \n",
    "   \n",
    "            ad_miledata=body_data.find(name='div', class_='col-lg-2')\n",
    "            #print(ad_miledata)\n",
    "            if ad_miledata != None :\n",
    "     \n",
    "                ad_mileage=ad_miledata.get_text().strip()[:-2]\n",
    "             \n",
    "            ad_linkdata = body_data.find(name='a')\n",
    "            #print ()\n",
    "            if ad_linkdata != None:\n",
    "                ad_linkdetails = base+ad_linkdata['href']\n",
    "            \n",
    "            desc_data = body_data.find(name='div', class_='col-lg-12')\n",
    "            if desc_data != None:\n",
    "                description = desc_data.get_text().strip()\n",
    "            \n",
    "            expired = body_data.find(name='div', class_='status_30')\n",
    "            \n",
    "            if expired != None:\n",
    "                ad_status='expired'\n",
    "            else :\n",
    "                ad_status='live'\n",
    "                \n",
    "            sold = body_data.find(name='div', class_='status_40')\n",
    "            \n",
    "            if sold != None:\n",
    "                ad_status='sold'\n",
    "                        \n",
    "            \n",
    "        # Storing data into dictionary\n",
    "        if header!=None :\n",
    "            \n",
    "            bike_dict['ad_header'] = ad_header\n",
    "            bike_dict['ad_price'] = ad_price\n",
    "            bike_dict['ad_registration'] = ad_registration\n",
    "            bike_dict['ad_mileage'] = ad_mileage\n",
    "            bike_dict['ad_biketype'] = ad_biketype\n",
    "            bike_dict['ad_description'] = description\n",
    "            bike_dict['ad_posted_on'] = ad_posted_on\n",
    "            bike_dict['ad_paid_flag'] = ad_paid_flag[0:len('Paid Ad')]\n",
    "            bike_dict['ad_direct_seller'] = ad_direct_seller\n",
    "            bike_dict['ad_dealer_seller'] = ad_dealer_seller\n",
    "            bike_dict['ad_status'] = ad_status\n",
    "            bike_dict['ad_linkdetails'] = ad_linkdetails\n",
    "            \n",
    "            i=1\n",
    "            for splitter in split_header:\n",
    "                bike_dict['header_'+str(i)]=splitter\n",
    "                i+=1\n",
    "\n",
    "        #print(bike_dict )\n",
    "        #print(\"\\n\")\n",
    "       \n",
    "        \n",
    "        # append the dictionary into a list \n",
    "        bike_list.append(bike_dict)\n",
    "        \n",
    "        #print(\"Counter \" + str(count_items))\n",
    "        count_items += 1\n",
    "    \n",
    "    print(\"Number of items scraped: \", count_items)\n",
    "    print(\"-\" * 40)\n",
    "        \n",
    "    # Retrieve new extension (next extension is for the Next page)\n",
    "    extension = bike_page.find('ul', class_='pagination')\n",
    "    pindex=extension.find('li', class_='active').get_text()\n",
    "    \n",
    "    #print(\"Index\" + pindex)\n",
    "    next_index=int(pindex)+1\n",
    "    extension = \"?page=\" + str(next_index) +\"&\"\n",
    "\n",
    "    #print(extension)\n",
    "    count_pages += 1\n",
    "    time.sleep(5)\n",
    "\n",
    "print(\"--Scraping done!--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Library "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas\n",
    " - DataFrame object for data manipulation and analysis.\n",
    " - Pandas is built on top of the __NumPy__ package\n",
    " \n",
    "### Functions\n",
    " - .head()\n",
    "     - Display top 5 data\n",
    " - .tail(2)\n",
    "     - Display bottom 2 data\n",
    " - .info()\n",
    "     - essential details about your dataset, such as the number of rows and columns, the number of non-null values, what type of data is in each column, and how much memory your DataFrame is using.\n",
    " - .shape()\n",
    "     - Display (rows, columns)\n",
    " - https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data (list of dictionary) to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(bike_list)\n",
    "df.to_csv('/Users/ganesha/Downloads/Antler/Bizoo/sgbikemartdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
